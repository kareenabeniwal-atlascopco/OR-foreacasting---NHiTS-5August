# Now make predictions using the correctly formatted future dataframe
print("Making predictions...")
try:
    forecasts = fcst.predict(futr_df=future_df_with_trend)
    print("✅ Prediction successful!")
    print(f"Forecasts shape: {forecasts.shape}")
    print(f"Forecasts columns: {list(forecasts.columns)}")
    print("\nFirst few forecast rows:")
    print(forecasts.head())
except Exception as e:
    print(f"❌ Prediction failed: {e}")
    print("\nLet's check if there are still missing combinations...")
    
    # Use the get_missing_future method to identify what's missing
    try:
        missing = fcst.get_missing_future(future_df_with_trend, df_train)
        if len(missing) > 0:
            print(f"Missing combinations: {len(missing)}")
            print(missing.head(10))
        else:
            print("No missing combinations found.")
    except Exception as e2:
        print(f"Error checking missing combinations: {e2}")# Now we need to add the 'trend' exogenous variable to the future dataframe
# We'll merge the trend values from our test data
print("Adding trend values to future dataframe...")

# Create a mapping of unique_id and ds to trend values from the original test data
trend_mapping = df_test[['unique_id', 'ds', 'trend']].copy()
print(f"Trend mapping shape: {trend_mapping.shape}")

# Merge the trend values into the future dataframe
future_df_with_trend = future_df.merge(
    trend_mapping, 
    on=['unique_id', 'ds'], 
    how='left'
)

print(f"Future dataframe with trend shape: {future_df_with_trend.shape}")
print("\nChecking for missing trend values:")
missing_trend = future_df_with_trend['trend'].isna().sum()
print(f"Missing trend values: {missing_trend}")

if missing_trend > 0:
    print("Rows with missing trend values:")
    print(future_df_with_trend[future_df_with_trend['trend'].isna()][['unique_id', 'ds']])

print("\nFinal future dataframe:")
print(future_df_with_trend.head())
print(f"Columns: {list(future_df_with_trend.columns)}")# Use the make_future_dataframe method to create the correct future dataframe
# This ensures all required ID-timestamp combinations are included
future_df = fcst.make_future_dataframe(df_train)
print("Future dataframe shape:", future_df.shape)
print("Future dataframe columns:", list(future_df.columns))
print("\nFirst few rows of future_df:")
print(future_df.head())

# Check the date range in future_df
print(f"\nFuture dataframe date range: {future_df['ds'].min()} to {future_df['ds'].max()}")
print(f"Number of unique IDs in future_df: {future_df['unique_id'].nunique()}")
print(f"Unique IDs: {sorted(future_df['unique_id'].unique())}")# Let's first check what combinations are expected vs what we have
print("Unique IDs in training data:", sorted(df_train['unique_id'].unique()))
print("Unique IDs in test data:", sorted(df_test['unique_id'].unique()))
print("\nTraining data shape:", df_train.shape)
print("Test data shape:", df_test.shape)

# Check if all regions have data for all test periods
test_periods = df_test['ds'].unique()
test_regions = df_test['unique_id'].unique()
print(f"\nTest periods: {len(test_periods)} periods")
print(f"Test regions: {len(test_regions)} regions")
print(f"Expected combinations: {len(test_periods) * len(test_regions)}")
print(f"Actual test data rows: {len(df_test)}")

# Check for missing combinations
missing_combinations = []
for region in df_train['unique_id'].unique():
    for period in test_periods:
        if not ((df_test['unique_id'] == region) & (df_test['ds'] == period)).any():
            missing_combinations.append((region, period))

if missing_combinations:
    print(f"\nMissing combinations: {len(missing_combinations)}")
    for region, period in missing_combinations[:10]:  # Show first 10
        print(f"  {region} - {period}")
    if len(missing_combinations) > 10:
        print(f"  ... and {len(missing_combinations) - 10} more")
else:
    print("\nNo missing combinations found!"){
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea0e104b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from neuralforecast import NeuralForecast\n",
    "from neuralforecast.models import NHITS\n",
    "from neuralforecast.losses.pytorch import DistributionLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ecade42",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"GMRegions_CPI.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07ec5aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns={\n",
    "    'GM Region': 'unique_id',\n",
    "    'Month-Year': 'ds',\n",
    "    'CPI': 'trend',\n",
    "    'EUR': 'y'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "afb191a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_id</th>\n",
       "      <th>ds</th>\n",
       "      <th>y</th>\n",
       "      <th>trend</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Berema</td>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>1.130630e+06</td>\n",
       "      <td>1.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Berema</td>\n",
       "      <td>2020-02-01</td>\n",
       "      <td>2.034458e+06</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Berema</td>\n",
       "      <td>2020-03-01</td>\n",
       "      <td>6.543391e+06</td>\n",
       "      <td>0.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Berema</td>\n",
       "      <td>2020-04-01</td>\n",
       "      <td>5.069735e+06</td>\n",
       "      <td>0.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Berema</td>\n",
       "      <td>2020-05-01</td>\n",
       "      <td>1.050716e+06</td>\n",
       "      <td>1.08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  unique_id         ds             y  trend\n",
       "0    Berema 2020-01-01  1.130630e+06   1.73\n",
       "1    Berema 2020-02-01  2.034458e+06   0.72\n",
       "2    Berema 2020-03-01  6.543391e+06   0.54\n",
       "3    Berema 2020-04-01  5.069735e+06   0.63\n",
       "4    Berema 2020-05-01  1.050716e+06   1.08"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "83792c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique series IDs\n",
    "unique_ids = df['unique_id'].unique()\n",
    "\n",
    "# One-hot encode without prefix\n",
    "static_features = pd.get_dummies(pd.DataFrame({'unique_id': unique_ids}),\n",
    "                                 columns=['unique_id'], prefix='', prefix_sep='').astype(int)\n",
    "\n",
    "# Add unique_id as the first column\n",
    "df_static = pd.DataFrame({'unique_id': unique_ids})\n",
    "df_static = pd.concat([df_static, static_features], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3f5e4aa3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_id</th>\n",
       "      <th>Berema</th>\n",
       "      <th>DACH and Benelux</th>\n",
       "      <th>Eastern Europe</th>\n",
       "      <th>France</th>\n",
       "      <th>Greater China</th>\n",
       "      <th>India</th>\n",
       "      <th>Liuzhou</th>\n",
       "      <th>MEN</th>\n",
       "      <th>Mexico and Central America</th>\n",
       "      <th>Scandinavia</th>\n",
       "      <th>South America</th>\n",
       "      <th>South Europe</th>\n",
       "      <th>UK and Ireland</th>\n",
       "      <th>US and Canada</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Berema</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DACH and Benelux</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Eastern Europe</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>France</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Greater China</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          unique_id  Berema  DACH and Benelux  Eastern Europe  France  \\\n",
       "0            Berema       1                 0               0       0   \n",
       "1  DACH and Benelux       0                 1               0       0   \n",
       "2    Eastern Europe       0                 0               1       0   \n",
       "3            France       0                 0               0       1   \n",
       "4     Greater China       0                 0               0       0   \n",
       "\n",
       "   Greater China  India  Liuzhou  MEN  Mexico and Central America  \\\n",
       "0              0      0        0    0                           0   \n",
       "1              0      0        0    0                           0   \n",
       "2              0      0        0    0                           0   \n",
       "3              0      0        0    0                           0   \n",
       "4              1      0        0    0                           0   \n",
       "\n",
       "   Scandinavia  South America  South Europe  UK and Ireland  US and Canada  \n",
       "0            0              0             0               0              0  \n",
       "1            0              0             0               0              0  \n",
       "2            0              0             0               0              0  \n",
       "3            0              0             0               0              0  \n",
       "4            0              0             0               0              0  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_static.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72a643ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training period: 2020-01-01 00:00:00 to 2024-12-01 00:00:00\n",
      "Test period: 2025-01-01 00:00:00 to 2025-03-01 00:00:00\n",
      "Number of series (regions): 14\n",
      "Train size: 840 rows  |  Test size: 42 rows\n"
     ]
    }
   ],
   "source": [
    "# Split data into training (up to 2024-12) and test (2025-01 to 2025-03)\n",
    "train_cutoff = pd.Timestamp('2025-01-01')\n",
    "df_train = df[df['ds'] < train_cutoff].copy()   # data up to Dec 2024\n",
    "df_test  = df[df['ds'] >= train_cutoff].copy()  # data from Jan 2025 onward\n",
    "\n",
    "print(\"Training period:\", df_train['ds'].min(), \"to\", df_train['ds'].max())\n",
    "print(\"Test period:\", df_test['ds'].min(), \"to\", df_test['ds'].max())\n",
    "print(\"Number of series (regions):\", df['unique_id'].nunique())\n",
    "print(\"Train size:\", len(df_train), \"rows  |  Test size:\", len(df_test), \"rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "02b4e219",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 1\n"
     ]
    }
   ],
   "source": [
    "model = NHITS(h=12,\n",
    "              input_size=24,\n",
    "              loss=DistributionLoss(distribution='StudentT', level=[80, 90], return_params=True),\n",
    "              stat_exog_list=['Berema'],\n",
    "              futr_exog_list=['trend'],\n",
    "              n_freq_downsample=[2, 1, 1],\n",
    "              scaler_type='robust',\n",
    "              max_steps=200,\n",
    "              early_stop_patience_steps=2,\n",
    "              inference_windows_batch_size=1,\n",
    "              val_check_steps=10,\n",
    "              learning_rate=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3d85f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fcst = NeuralForecast(models=[model], freq='MS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "65ad62d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1️⃣ Prepare futr_df for prediction\n",
    "futr_df = df_test[['unique_id', 'ds', 'trend']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049366bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "fcst.fit(df=df_train, static_df=df_static, val_size=12)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e45daa88",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "There are missing combinations of ids and times in `futr_df`.\nYou can run the `make_future_dataframe()` method to get the expected combinations or the `get_missing_future(futr_df)` method to get the missing combinations.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m forecasts \u001b[38;5;241m=\u001b[39m \u001b[43mfcst\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfutr_df\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdf_test\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\neuralforecast\\core.py:933\u001b[0m, in \u001b[0;36mNeuralForecast.predict\u001b[1;34m(self, df, static_df, futr_df, verbose, engine, level, quantiles, **data_kwargs)\u001b[0m\n\u001b[0;32m    931\u001b[0m         expected_cmd \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmake_future_dataframe(df)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    932\u001b[0m         missing_cmd \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mget_missing_future(futr_df, df)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 933\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    934\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThere are missing combinations of ids and times in `futr_df`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    935\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou can run the `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexpected_cmd\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` method to get the expected combinations or \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    936\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmissing_cmd\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` method to get the missing combinations.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    937\u001b[0m     )\n\u001b[0;32m    938\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m futr_orig_rows \u001b[38;5;241m>\u001b[39m futr_df\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]:\n\u001b[0;32m    939\u001b[0m     dropped_rows \u001b[38;5;241m=\u001b[39m futr_orig_rows \u001b[38;5;241m-\u001b[39m futr_df\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mValueError\u001b[0m: There are missing combinations of ids and times in `futr_df`.\nYou can run the `make_future_dataframe()` method to get the expected combinations or the `get_missing_future(futr_df)` method to get the missing combinations."
     ]
    }
   ],
   "source": [
    "# FIXED: Use make_future_dataframe to ensure all required ID-timestamp combinations
# Original error was due to missing combinations in futr_df
future_df = fcst.make_future_dataframe(df_train)

# Add the trend exogenous variable from test data
trend_mapping = df_test[['unique_id', 'ds', 'trend']].copy()
future_df_with_trend = future_df.merge(trend_mapping, on=['unique_id', 'ds'], how='left')

# Make predictions with the correctly formatted future dataframe
forecasts = fcst.predict(futr_df=future_df_with_trend)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
